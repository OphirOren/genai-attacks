{
  "$id": "$gai-technique/citation_manipulation",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "The adversary manipulates citations provided by the AI system to add trustworthiness to their social engineering attack. Variants include providing the wrong citation, making up a new one or providing the right citation for the wrong data.",
  "external_references": [],
  "framework_references": [],
  "name": "Citation Manipulation",
  "object_references": [
    {
      "$id": "$gai-tactic/impact",
      "$type": "tactic",
      "description": "An adversary can social engineer by providing trustworthy sources to maliciously-crafted messages or data."
    },
    {
      "$id": "$gai-technique/citation_silencing",
      "$type": "technique",
      "description": "An adjacent technique which also includes adversary control over citations."
    }
  ]
}
